# -*- coding: utf-8 -*-
"""agnos-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6t6J58vrjJfNFeHc4w38R0N3SViNwOt
"""

!pip install beautifulsoup4 requests

from google.colab import drive
drive.mount('/content/drive')

!cp forum_threads.json /content/drive/MyDrive/agnos-assignment/

!pip install langchain faiss-cpu sentence-transformers transformers

!pip install -U langchain langchain-community

!pip install beautifulsoup4 requests tqdm

import requests
from bs4 import BeautifulSoup
import json
import os
from tqdm import tqdm

def scrape_forum_posts(base_url="https://www.agnoshealth.com/forums"):
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    threads = []
    for thread in soup.find_all('div', class_='forum-thread'):
        try:
            title = thread.find('h2').text.strip()
            url = thread.find('a')['href']
            full_url = url if url.startswith('http') else f"https://www.agnoshealth.com{url}"
            threads.append({"title": title, "url": full_url})
        except:
            continue

    posts = []
    for thread in tqdm(threads):
        try:
            thread_page = requests.get(thread['url'])
            thread_soup = BeautifulSoup(thread_page.text, 'html.parser')
            post_content = thread_soup.find('div', class_='post-body').text.strip()
            posts.append({
                "title": thread['title'],
                "url": thread['url'],
                "content": post_content
            })
        except:
            continue

    os.makedirs('/content/drive/MyDrive/agnos-assignment', exist_ok=True)

    with open('/content/drive/MyDrive/agnos-assignment/forum_posts_full.json', 'w', encoding='utf-8') as f:
        json.dump(posts, f, ensure_ascii=False, indent=4)

    print(f"Scraped {len(posts)} posts!")

scrape_forum_posts()

from google.colab import drive
drive.mount('/content/drive')

!pip install beautifulsoup4 requests tqdm


import requests
from bs4 import BeautifulSoup
import json
import os
from tqdm import tqdm


def scrape_forum_posts(base_url="https://www.agnoshealth.com/forums"):
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    threads = []
    for link in soup.find_all('a', href=True):
        href = link['href']
        if "/forums/" in href and not href.endswith('/forums/'):
            full_url = href if href.startswith('http') else f"https://www.agnoshealth.com{href}"
            title = link.text.strip()
            threads.append({"title": title, "url": full_url})

    print(f" ‡πÄ‡∏à‡∏≠‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(threads)} ‡∏Å‡∏£‡∏∞‡∏ó‡∏π‡πâ")

    posts = []
    for thread in tqdm(threads):
        try:
            thread_page = requests.get(thread['url'])
            thread_soup = BeautifulSoup(thread_page.text, 'html.parser')


            possible_tags = thread_soup.find_all(['div', 'article', 'section', 'p'])
            full_text = " ".join(tag.get_text(strip=True) for tag in possible_tags)

            if len(full_text) > 200:
                posts.append({
                    "title": thread['title'],
                    "url": thread['url'],
                    "content": full_text
                })
        except Exception as e:
            print(f" Error ‡∏ó‡∏µ‡πà {thread['url']}: {e}")
            continue


    os.makedirs('/content/drive/MyDrive/agnos-assignment', exist_ok=True)

    with open('/content/drive/MyDrive/agnos-assignment/forum_posts_full.json', 'w', encoding='utf-8') as f:
        json.dump(posts, f, ensure_ascii=False, indent=4)

    print(f" Scraped ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡πâ‡∏ß {len(posts)} ‡πÇ‡∏û‡∏™‡∏ï‡πå!")


scrape_forum_posts()

!pip install -U langchain langchain-community faiss-cpu sentence-transformers transformers


from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import json


from google.colab import drive
drive.mount('/content/drive')


with open('/content/drive/MyDrive/agnos-assignment/forum_posts_full.json', 'r', encoding='utf-8') as f:
    posts = json.load(f)

print(f"Loaded {len(posts)} posts.")


documents = [post["title"] + "\n" + post["content"] for post in posts]

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50
)

chunks = []
for doc in documents:
    splits = splitter.split_text(doc)
    chunks.extend(splits)

print(f"Split into {len(chunks)} chunks.")


embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.from_texts(chunks, embeddings)

print("FAISS database built with small chunks!")

model_name = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)


qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

e
llm = HuggingFacePipeline(
    pipeline=qa_pipeline
)

print("LLM Flan-T5-large loaded!")


qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 1}),
    chain_type="stuff"
)

print(" RAG Pipeline ready!")


query = "What are the early symptoms of diabetes?"
result = qa.run(query)

print("\n\n--- Test Result ---")
print("Q:", query)
print("A:", result)

questions = [
    "What are the early symptoms of diabetes?",
    "How to naturally lower cholesterol levels?",
    "What are the side effects of taking aspirin daily?",
    "How can I improve my sleep quality without medication?",
    "Which foods should be avoided for high blood pressure?",
    "What are common treatments for chronic back pain?",
    "What is a healthy diet plan for losing weight?",
    "How does chronic stress affect heart health?",
    "What are warning signs of a stroke?",
    "Is it safe to exercise if you have asthma?"
]

def batch_test_rag(qa, questions):
    for idx, q in enumerate(questions):
        print(f"\n--- Question {idx+1}: {q} ---")
        try:
            answer = qa.run(q)
            print(f"Answer:\n{answer}\n")
        except Exception as e:
            print(f"Error: {str(e)}\n")

batch_test_rag(qa, questions)

!pip install streamlit langchain langchain-community faiss-cpu sentence-transformers transformers

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import json
import os

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• forum posts
with open('/content/drive/MyDrive/agnos-assignment/forum_posts_full.json', 'r', encoding='utf-8') as f:
    posts = json.load(f)

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
documents = [post["title"] + " " + post["content"] for post in posts]

# ‡πÇ‡∏´‡∏•‡∏î Embeddings model
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á FAISS vectorstore
db = FAISS.from_texts(documents, embeddings)

# ‡πÄ‡∏ã‡∏ü FAISS
save_path = "/content/drive/MyDrive/agnos-assignment/faiss_index"
os.makedirs(save_path, exist_ok=True)
db.save_local(save_path)

print("‚úÖ FAISS Database ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß!")

import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# --- Load FAISS ‡πÅ‡∏•‡∏∞ Model ---

# Embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ‡πÇ‡∏´‡∏•‡∏î FAISS (‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà build ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏µ‡πâ)
db = FAISS.load_local(
    "/content/drive/MyDrive/agnos-assignment/faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# ‡πÇ‡∏´‡∏•‡∏î Model Flan-T5 (‡∏´‡∏£‡∏∑‡∏≠ gpt2-medium ‡∏ñ‡πâ‡∏≤ RAM ‡∏ô‡πâ‡∏≠‡∏¢)
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Pipeline
qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(
    pipeline=qa_pipeline
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 1}),
    chain_type="stuff"
)

# --- Build Streamlit Chatbot UI ---

st.set_page_config(page_title="Agnos Health Chatbot", page_icon="üí¨")
st.title("üí¨ Agnos Health Chatbot")
st.caption("Chatbot assistant powered by RAG and Flan-T5-large")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            response = qa.run(prompt)
            st.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})

!streamlit run app.py

import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

db = FAISS.load_local(
    "/content/drive/MyDrive/agnos-assignment/faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)


model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(
    pipeline=qa_pipeline
)


qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=db.as_retriever(search_kwargs={"k": 2}),
    return_source_documents=False
)



st.set_page_config(page_title="Agnos Health Chatbot", page_icon="üí¨")
st.title("üí¨ Agnos Health Chatbot")
st.caption("Chatbot assistant powered by RAG + Conversational Memory")


if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


if user_input := st.chat_input("‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ..."):

    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})


    with st.chat_message("assistant"):
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
            result = qa_chain({"question": user_input, "chat_history": st.session_state.chat_history})
            response = result["answer"]
            st.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
    st.session_state.chat_history.append((user_input, response))

import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import tempfile
import os



def create_vectorstore_from_pdf(uploaded_file):

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name


    loader = PyMuPDFLoader(tmp_path)
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.split_documents(documents)


    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


    db = FAISS.from_documents(docs, embeddings)
    return db



model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(
    pipeline=qa_pipeline
)



st.set_page_config(page_title="Chat with Your PDF", page_icon="üìÑ")
st.title("üìÑ PDF Chatbot + Forum Chat")
st.caption("Chat with uploaded PDF or forum data. Powered by RAG + Flan-T5.")


uploaded_file = st.file_uploader("üìÑ Upload a PDF to chat with", type=["pdf"])


default_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
default_db_path = "/content/drive/MyDrive/agnos-assignment/faiss_index"


if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None


if uploaded_file is not None:
    db = create_vectorstore_from_pdf(uploaded_file)
    st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=db.as_retriever(search_kwargs={"k": 2}),
        return_source_documents=False
    )
    st.success("‚úÖ PDF uploaded and ready to chat!")
else:
    db = FAISS.load_local(
        default_db_path,
        default_embeddings,
        allow_dangerous_deserialization=True
    )
    st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=db.as_retriever(search_kwargs={"k": 2}),
        return_source_documents=False
    )


if st.button("üîÑ Reset Chat"):
    st.session_state.messages = []
    st.session_state.chat_history = []
    st.experimental_rerun()


for message in st.session_state.messages:
    role = "üë§ You" if message["role"] == "user" else "ü§ñ Bot"
    bubble_color = "#DCF8C6" if message["role"] == "user" else "#E6E6FA"
    with st.container():
        st.markdown(
            f'<div style="background-color: {bubble_color}; padding: 10px; border-radius: 10px; margin: 5px 0;">'
            f'<b>{role}:</b> {message["content"]}</div>',
            unsafe_allow_html=True
        )


if user_input := st.chat_input("‡∏ñ‡∏≤‡∏°‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ..."):
    with st.container():
        st.markdown(
            f'<div style="background-color: #DCF8C6; padding: 10px; border-radius: 10px; margin: 5px 0;">'
            f'<b>üë§ You:</b> {user_input}</div>',
            unsafe_allow_html=True
        )
    st.session_state.messages.append({"role": "user", "content": user_input})


    with st.spinner("ü§î ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
        result = st.session_state.qa_chain({
            "question": user_input,
            "chat_history": st.session_state.chat_history
        })
        response = result["answer"]

    with st.container():
        st.markdown(
            f'<div style="background-color: #E6E6FA; padding: 10px; border-radius: 10px; margin: 5px 0;">'
            f'<b>ü§ñ Bot:</b> {response}</div>',
            unsafe_allow_html=True
        )
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.session_state.chat_history.append((user_input, response))

!pip install gradio langchain langchain-community faiss-cpu sentence-transformers transformers


import gradio as gr
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM


embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local(
    "/content/drive/MyDrive/agnos-assignment/faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)


model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(
    pipeline=qa_pipeline
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(search_kwargs={"k": 2}),
    chain_type="stuff"
)



def chatbot_fn(user_input):
    if not user_input:
        return "‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô"

    result = qa.run(user_input)
    return result

iface = gr.Interface(
    fn=chatbot_fn,
    inputs=gr.Textbox(lines=2, placeholder="‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏≠‡∏∞‡πÑ‡∏£‡∏Å‡πá‡πÑ‡∏î‡πâ..."),
    outputs="text",
    title="Agnos Health Chatbot üí¨",
    description="‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û"
)

iface.launch(share=True)

import gradio as gr
import os
import shutil
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM


model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

qa_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(pipeline=qa_pipeline)


vector_db = None
messages = []


def create_vectorstore_from_pdfs(pdf_files):
    global vector_db


    if os.path.exists("pdfs"):
        shutil.rmtree("pdfs")
    os.makedirs("pdfs", exist_ok=True)


    for uploaded_file in pdf_files:
        file_path = os.path.join("pdfs", uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())


    docs = []
    for filename in os.listdir("pdfs"):
        loader = PyPDFLoader(os.path.join("pdfs", filename))
        docs.extend(loader.load())


    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    documents = text_splitter.split_documents(docs)


    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = FAISS.from_documents(documents, embeddings)

    return f"‚úÖ Loaded {len(pdf_files)} PDFs successfully!"


def chatbot_fn(user_input):
    global vector_db, messages

    if vector_db is None:
        return "‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î PDF ‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö", messages

    retriever = vector_db.as_retriever(search_kwargs={"k": 2})
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")

    answer = qa.run(user_input)


    messages.append(("user", user_input))
    messages.append(("bot", answer))


    chat_bubbles = []
    for role, text in messages:
        if role == "user":
            chat_bubbles.append(gr.ChatMessage(text=text, name="‡∏Ñ‡∏∏‡∏ì", avatar="üë§"))
        else:
            chat_bubbles.append(gr.ChatMessage(text=text, name="AgnosBot", avatar="ü§ñ"))

    return "", chat_bubbles


def reset_chat():
    global messages
    messages = []
    return [], "üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß!"


def save_chat():
    global messages
    save_text = ""
    for role, text in messages:
        if role == "user":
            save_text += f"[‡∏Ñ‡∏∏‡∏ì]: {text}\n"
        else:
            save_text += f"[AgnosBot]: {text}\n"

    with open("chat_history.txt", "w", encoding="utf-8") as f:
        f.write(save_text)

    return "chat_history.txt"


with gr.Blocks(title="Agnos Health Chatbot") as app:
    gr.Markdown("# üí¨ Agnos Health Chatbot")
    gr.Markdown("‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å PDF ‡∏´‡∏£‡∏∑‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á üöÄ")

    with gr.Row():
        pdf_upload = gr.File(label="üìÑ Upload PDFs", file_types=[".pdf"], file_count="multiple")
        upload_btn = gr.Button("üì• Load PDFs")

    status = gr.Label()

    chatbot = gr.Chatbot(label="AgnosBot Chat")
    user_input = gr.Textbox(placeholder="‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û...", show_label=False)

    with gr.Row():
        send_btn = gr.Button("üí¨ Send")
        reset_btn = gr.Button("üóëÔ∏è Reset Chat")
        save_btn = gr.Button("üíæ Save Chat")

    download = gr.File(label="‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")

    upload_btn.click(create_vectorstore_from_pdfs, inputs=[pdf_upload], outputs=[status])
    send_btn.click(chatbot_fn, inputs=[user_input], outputs=[user_input, chatbot])
    reset_btn.click(reset_chat, outputs=[chatbot, status])
    save_btn.click(save_chat, outputs=[download])

app.launch(share=True)